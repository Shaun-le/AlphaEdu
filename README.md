# A New Vietnamese Dataset for Question Answer Generation in Education

## Abstract
Large-scale and high-quality corpora are essential for evaluating question-answer generation (QAG) models, especially in low-resource languages such as Vietnamese.
Within the education domain, QAG holds significant potential for practical applications; however, research in this field remains limited.
To fill the gap, this paper introduces ViEduQA, a novel dataset designed to advance QAG in Vietnamese educational contexts.
The dataset, covering four high school subjects (Biology, Geography, History, and Civic Education), consists of a QAG task that generates question-answer pairs directly from educational texts.
We explore the capabilities of pre-trained language models (PLMs) like ViT5 and BARTPho and large language models (LLMs) such as SeaLLMs and Qwen2, leveraging fine-tuning and instruction-tuning methods.
Finally, our analysis shows that the instruction tuning method has the potential to enhance the performance of QAG models, though it requires additional data. We also provide a system demonstrating how our QAG models function in education.

## Data

## Citation
```
@InProceedings{10.1007/978-981-96-4288-5_34,
author="Nguyen, Truong-Phuc
and Le, Huu-Loi",
editor="Buntine, Wray
and Fjeld, Morten
and Tran, Truyen
and Tran, Minh-Triet
and Huynh Thi Thanh, Binh
and Miyoshi, Takumi",
title="ViEduQA: A New Vietnamese Dataset for Question Answer Generation in Education",
booktitle="Information and Communication Technology",
year="2025",
publisher="Springer Nature Singapore",
address="Singapore",
pages="441--455",
abstract="Large-scale and high-quality corpora are essential for evaluating question-answer generation (QAG) models, especially in low-resource languages such as Vietnamese. Within the education domain, QAG holds significant potential for practical applications; however, research in this field remains limited. To fill the gap, this paper introduces ViEduQA, a novel dataset designed to advance QAG in Vietnamese educational contexts. The dataset, covering four high school subjects (Biology, Geography, History, and Civic Education), consists of a QAG task that generates question-answer pairs directly from educational texts. We explore the capabilities of pre-trained language models (PLMs) like ViT5 and BARTPho and large language models (LLMs) such as SeaLLMs and Qwen2, leveraging fine-tuning and instruction-tuning methods. Finally, our analysis shows that the instruction tuning method has the potential to enhance the performance of QAG models, though it requires additional data. We also provide a system demonstrating how our QAG models function in education.",
isbn="978-981-96-4288-5"
}


```